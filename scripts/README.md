# ğŸ—ï¸ Lakehouse Data Ingestion with MinIO

HÆ°á»›ng dáº«n chi tiáº¿t cÃ¡ch lÃ m viá»‡c vá»›i MinIO Ä‘á»ƒ ingest dá»¯ liá»‡u vÃ o Bronze layer cá»§a lakehouse.

## ğŸ“‹ Má»¥c lá»¥c
- [MinIO Console Access](#minio-console-access)
- [Cáº¥u trÃºc Bucket](#cáº¥u-trÃºc-bucket)
- [Chuáº©n bá»‹ Environment](#chuáº©n-bá»‹-environment)
- [Scripts Available](#scripts-available)
- [Quick Start](#quick-start)
- [Manual Operations](#manual-operations)
- [Troubleshooting](#troubleshooting)

## ğŸŒ MinIO Console Access

### Web Console
```
URL: http://localhost:9000
Username: minioadmin
Password: minioadmin
```

### Kiá»ƒm tra MinIO Ä‘ang cháº¡y
```bash
docker ps | findstr minio
```

## ğŸ“ Cáº¥u trÃºc Bucket

Lakehouse sá»­ dá»¥ng 3 buckets chÃ­nh theo Medallion Architecture:

```
bronze/                    # Raw data landing zone
â”œâ”€â”€ customers/            # Customer data
â”œâ”€â”€ products/             # Product catalog
â”œâ”€â”€ orders/               # Order transactions
â”œâ”€â”€ fx_rates/             # Exchange rates
â””â”€â”€ archive/              # Historical archives

silver/                   # Cleaned & transformed data
â”œâ”€â”€ customers/            # Standardized customers
â”œâ”€â”€ products/             # Enriched products
â”œâ”€â”€ orders/               # Business-validated orders
â”œâ”€â”€ analytics/            # Analytical views
â””â”€â”€ staging/              # Transformation staging

gold/                     # Business-ready analytics
â”œâ”€â”€ reports/              # Business reports
â”œâ”€â”€ dashboards/           # Dashboard data
â”œâ”€â”€ metrics/              # KPIs & metrics
â””â”€â”€ exports/              # Data exports
```

## ğŸ› ï¸ Chuáº©n bá»‹ Environment

### 1. Install Dependencies
```bash
pip install boto3 pandas pyarrow faker
```

### 2. Verify MinIO Connection
```python
from scripts.minio_manager import MinIOManager
minio = MinIOManager()
print("Buckets:", minio.get_bucket_info())
```

## ğŸ“œ Scripts Available

### 1. `minio_manager.py` - MinIO Operations
```python
from scripts.minio_manager import MinIOManager

# Initialize
minio = MinIOManager()

# Setup lakehouse structure
minio.setup_lakehouse_structure()

# Upload file
minio.upload_file('bronze', 'customers/data.parquet', 'local_file.parquet')

# Upload DataFrame
minio.upload_dataframe('bronze', 'customers/data.parquet', df)

# Download file
minio.download_file('bronze', 'customers/data.parquet', 'local_copy.parquet')

# Download DataFrame
df = minio.download_dataframe('bronze', 'customers/data.parquet')

# List objects
objects = minio.list_objects('bronze', prefix='customers/')
```

### 2. `mock_data_generator.py` - Generate Sample Data
```python
from scripts.mock_data_generator import EcommerceDataGenerator

# Initialize generator
generator = EcommerceDataGenerator()

# Generate complete dataset
data = generator.generate_all_data(scale='small')  # small/medium/large

# Access individual tables
customers_df = data['customers']
products_df = data['products']
orders_df = data['orders']
fx_rates_df = data['fx_rates']

# Save to Parquet files
generator.save_to_parquet(data, output_dir='data/mock')
```

**Data Scales:**
- `small`: 1K customers, 500 products, 5K orders
- `medium`: 10K customers, 5K products, 50K orders
- `large`: 50K customers, 25K products, 250K orders

### 3. `data_ingestion_demo.py` - Complete Demo
```bash
# Setup MinIO structure only
python scripts/data_ingestion_demo.py --setup-only

# Full demo with small data
python scripts/data_ingestion_demo.py --scale small

# Medium scale demo
python scripts/data_ingestion_demo.py --scale medium
```

## ğŸš€ Quick Start

### Step 1: Setup MinIO Structure
```bash
cd /path/to/lakehouse
python scripts/data_ingestion_demo.py --setup-only
```

### Step 2: Generate & Upload Sample Data
```bash
python scripts/data_ingestion_demo.py --scale small
```

### Step 3: Verify in MinIO Console
1. Open http://localhost:9000
2. Login vá»›i minioadmin/minioadmin
3. Check buckets: bronze, silver, gold
4. Browse files trong bronze/customers/, bronze/products/, etc.

## ğŸ”§ Manual Operations

### Upload File tá»« Command Line
```bash
# Using AWS CLI (configure for MinIO first)
aws --endpoint-url=http://localhost:9000 s3 cp data.parquet s3://bronze/customers/

# Using mc (MinIO Client)
mc alias set lakehouse http://localhost:9000 minioadmin minioadmin
mc cp data.parquet lakehouse/bronze/customers/
```

### Create Bucket Structure Manually
```python
from scripts.minio_manager import MinIOManager

minio = MinIOManager()

# Create buckets
minio.create_buckets()

# Create folders
folders = ['customers/', 'products/', 'orders/', 'fx_rates/', 'archive/']
for folder in folders:
    minio.s3_client.put_object(Bucket='bronze', Key=folder)
```

### Upload Existing CSV/Parquet Files
```python
import pandas as pd
from scripts.minio_manager import MinIOManager

minio = MinIOManager()

# Upload CSV file
df = pd.read_csv('existing_data.csv')
minio.upload_dataframe('bronze', 'customers/existing_data.parquet', df)

# Upload existing Parquet
minio.upload_file('bronze', 'products/catalog.parquet', 'existing_catalog.parquet')
```

## ğŸ” Data Organization Best Practices

### File Naming Convention
```
{table_name}_{YYYYMMDD_HHMMSS}.parquet
customers_20241201_143022.parquet
orders_20241201_143500.parquet
```

### Partitioning Strategy
```
bronze/orders/
â”œâ”€â”€ year=2024/
â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”œâ”€â”€ orders_20240101_*.parquet
â”‚   â”‚   â””â”€â”€ orders_20240102_*.parquet
â”‚   â””â”€â”€ month=02/
â”‚       â””â”€â”€ ...
â””â”€â”€ year=2023/
    â””â”€â”€ ...
```

### Metadata Standards
```python
metadata = {
    'source': 'mock_generator',
    'table': 'customers',
    'rows': len(df),
    'columns': list(df.columns),
    'created_at': datetime.now().isoformat(),
    'version': '1.0'
}

minio.upload_dataframe('bronze', key, df, metadata=metadata)
```

## ğŸš¨ Troubleshooting

### Connection Issues
```bash
# Check MinIO container
docker ps | grep minio

# Check MinIO logs
docker logs minio

# Test connection
curl http://localhost:9000/minio/health/live
```

### Permission Errors
```python
# Check credentials
minio = MinIOManager(
    endpoint="http://localhost:9000",
    access_key="minioadmin",
    secret_key="minioadmin"
)
```

### Upload Failures
```python
# Check bucket exists
buckets = minio.get_bucket_info()
print("Available buckets:", [b['bucket'] for b in buckets])

# Test small upload first
test_df = pd.DataFrame({'test': [1, 2, 3]})
minio.upload_dataframe('bronze', 'test.parquet', test_df)
```

### Memory Issues with Large Files
```python
# For large datasets, process in chunks
chunk_size = 10000
for i, chunk in enumerate(pd.read_csv('large_file.csv', chunksize=chunk_size)):
    key = f'bronze/data/part_{i:04d}.parquet'
    minio.upload_dataframe('bronze', key, chunk)
```

## ğŸ“Š Monitoring & Validation

### Check Data in MinIO
```python
# List all objects
objects = minio.list_objects('bronze')
print(f"Total objects in bronze: {len(objects)}")

# Get bucket statistics
bucket_info = minio.get_bucket_info()
for info in bucket_info:
    print(f"{info['bucket']}: {info['objects']} objects, {info['size_mb']} MB")
```

### Validate Data Quality
```python
# Download and inspect sample
df = minio.download_dataframe('bronze', 'customers/customers_20241201.parquet')
print(f"Shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
print(f"Sample:\n{df.head()}")
print(f"Data types:\n{df.dtypes}")
```

## ğŸ”— Next Steps

Sau khi hoÃ n thÃ nh Bronze layer:

1. **Silver Layer**: Transform data vá»›i dbt
   ```bash
   cd dbt/
   dbt run --models staging
   ```

2. **Gold Layer**: Create analytics views
   ```bash
   dbt run --models marts
   ```

3. **BI Integration**: Connect vá»›i Superset/Metabase
4. **Orchestration**: Setup Airflow DAGs

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check MinIO logs: `docker logs minio`
2. Verify network: `curl http://localhost:9000`
3. Test scripts individually
4. Check Python dependencies

Happy data engineering! ğŸ‰
