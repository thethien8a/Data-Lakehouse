# ğŸ—ï¸ Data Lakehouse Project

*Building a Modern Data Platform with DuckDB, dbt, and MinIO*

## ğŸ“‹ Table of Contents
- [ğŸ¯ Overview](#-overview)
- [ğŸ› ï¸ Prerequisites](#ï¸-prerequisites)
- [ğŸš€ Setup Instructions](#-setup-instructions)
- [ğŸ“Š Data Ingestion Process](#-data-ingestion-process)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“ˆ Usage](#-usage)
- [ğŸ“ Documentation](#-documentation)

## ğŸ¯ Overview

This project implements a modern data lakehouse architecture using:
- **Object Storage**: MinIO (S3-compatible) for raw data lake storage
- **Compute Engine**: DuckDB for fast analytical queries
- **Transformation**: dbt-duckdb for data modeling & transformation
- **Data Quality**: Soda Core for automated quality checks
- **BI Tool**: Apache Superset / Metabase for data visualization

The project follows the **Medallion Architecture** with three layers:
- ğŸ¥‰ **Bronze Layer**: Raw, unprocessed data from sources
- ğŸ¥ˆ **Silver Layer**: Validated, standardized, and enriched data
- ğŸ¥‡ **Gold Layer**: Business-ready analytics and reporting

## ğŸ› ï¸ Prerequisites

- Docker and Docker Compose
- Python 3.8+
- pip package manager

## ğŸš€ Setup Instructions

### 1. Clone the Repository
```bash
git clone <repository-url>
cd lakehouse-project
```

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 3. Start MinIO Server
```bash
docker-compose -f docker/docker-compose.yml up -d
```

Wait for MinIO to be ready (approximately 30 seconds), then access the MinIO Console at `http://localhost:9001` with credentials:
- Access Key: `minioadmin`
- Secret Key: `minioadmin123`

### 4. Verify MinIO Buckets
After startup, MinIO will automatically create the following buckets:
- `bronze` - Raw data storage
- `silver` - Cleaned data storage
- `gold` - Analytics data storage

## ğŸ“Š Data Ingestion Process

### Step 1: Download UCI Online Retail II Dataset
```bash
python scripts/download_data.py
```

This script will:
- Download the UCI Online Retail II dataset from the official source
- Extract the Excel file
- Display information about the dataset structure

### Step 2: Ingest Data into MinIO
```bash
python scripts/ingest_data.py
```

This script will:
- Load the Excel file with multiple sheets
- Convert each sheet to Parquet format
- Upload the Parquet files to the `bronze` bucket in MinIO
- Add timestamp to filenames for versioning

## ğŸ“ Project Structure

```
lakehouse-project/
â”œâ”€â”€ ğŸ³ docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ minio/
â”œâ”€â”€ ğŸ’¾ data/
â”‚   â”œâ”€â”€ bronze/          # Raw data
â”‚   â”œâ”€â”€ silver/          # Cleaned data
â”‚   â””â”€â”€ gold/            # Analytics data
â”œâ”€â”€ ğŸ”„ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/     # Bronze â†’ Silver
â”‚   â”‚   â”œâ”€â”€ intermediate/ # Silver processing
â”‚   â”‚   â””â”€â”€ marts/       # Gold layer
â”‚   â”œâ”€â”€ tests/           # Data quality tests
â”‚   â”œâ”€â”€ macros/          # Reusable functions
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ âœ… soda/
â”‚   â”œâ”€â”€ configuration.yml
â”‚   â”œâ”€â”€ checks.yml
â”‚   â””â”€â”€ data_source.yml
â”œâ”€â”€ ğŸ“œ scripts/
â”‚   â”œâ”€â”€ download_data.py # Download UCI dataset
â”‚   â”œâ”€â”€ ingest_data.py   # Load data to MinIO
â”‚   â””â”€â”€ generate_data.py # Mock data generator
â”œâ”€â”€ ğŸ“ documents/
â”‚   â””â”€â”€ plan.md          # Project plan
â”œâ”€â”€ ğŸ““ notebooks/        # Exploratory analysis
â”œâ”€â”€ ğŸ¯ airflow/dags/     # Workflow orchestration
â””â”€â”€ requirements.txt     # Python dependencies
```

## ğŸ”§ Configuration

### MinIO Configuration
The MinIO server is configured with:
- Endpoint: `http://localhost:9000`
- Console: `http://localhost:9001`
- Access Key: `minioadmin`
- Secret Key: `minioadmin123`
- Default buckets: `bronze`, `silver`, `gold`

### Data Ingestion Configuration
The ingestion process is configured in `scripts/ingest_data.py`:
- Source: Excel file from UCI Online Retail II dataset
- Format: Parquet (for efficient storage and query performance)
- Destination: MinIO `bronze` bucket
- Naming convention: `online_retail_ii/{sheet_name}_{timestamp}.parquet`

## ğŸ“ˆ Usage

### Manual Data Ingestion
1. Download the dataset: `python scripts/download_data.py`
2. Ingest to MinIO: `python scripts/ingest_data.py`

### Data Pipeline Flow
```
Raw Data Sources â†’ Bronze Layer (MinIO) â†’ Silver Layer (dbt) â†’ Gold Layer (Analytics)
```

### Monitoring
- Access MinIO Console at `http://localhost:9001` to monitor data storage
- Check logs from data ingestion scripts for any errors
- Use dbt logs to monitor transformation processes

## ğŸ“ Documentation

- [Project Plan](documents/plan.md) - Detailed project architecture and implementation phases
- Script documentation is included in each Python file

## ğŸš€ Next Steps

After data ingestion, you can proceed with:
1. **Data Transformation**: Use dbt to transform Bronze â†’ Silver layer
2. **Data Quality**: Implement Soda Core checks
3. **Analytics**: Build Gold layer models and dashboards
4. **Orchestration**: Schedule pipelines with Airflow

---

*This project follows the principles of a modern data lakehouse architecture, enabling scalable and efficient data processing for analytics and machine learning workloads.*
